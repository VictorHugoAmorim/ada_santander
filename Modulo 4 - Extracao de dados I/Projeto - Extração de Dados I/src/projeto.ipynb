{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enunciado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projeto - Extração de Dados I\n",
    "------------------------------\n",
    "## Sistema de Monitoramento de Avanços no Campo da Genômica  \n",
    "\n",
    "## Contexto:  \n",
    "O grupo trabalha no time de engenharia de dados na HealthGen, uma empresa especializada em genômica e pesquisa de medicina personalizada. A genômica é o estudo do conjunto completo de genes de um organismo, desempenha um papel fundamental na medicina personalizada e na pesquisa biomédica. Permite a análise do DNA para identificar variantes genéticas e mutações associadas a doenças e facilita a personalização de tratamentos com base nas características genéticas individuais dos pacientes.\n",
    "\n",
    "A empresa precisa se manter atualizada sobre os avanços mais recentes na genômica, identificar oportunidades para pesquisa e desenvolvimento de tratamentos personalizados e acompanhar as tendências em genômica que podem influenciar estratégias de pesquisa e desenvolvimento. Pensando nisso, o time de dados apresentou uma proposta de desenvolvimento de um sistema que coleta, analisa e apresenta as últimas notícias relacionadas à genômica e à medicina personalizada, e também estuda o avanço do campo nos últimos anos.\n",
    "\n",
    "O time de engenharia de dados tem como objetivo desenvolver e garantir um pipeline de dados confiável e estável. As principais atividades são:\n",
    "\n",
    "### 1. Consumo de dados com a News API:  \n",
    "Implementar um mecanismo para consumir dados de notícias de fontes confiáveis e especializadas em genômica e medicina personalizada, a partir da News API:  \n",
    "https://newsapi.org/\n",
    "\n",
    "### 2. Definir Critérios de Relevância:  \n",
    "Desenvolver critérios precisos de relevância para filtrar as notícias. Por exemplo, o time pode se concentrar em notícias que mencionem avanços em sequenciamento de DNA, terapias genéticas personalizadas ou descobertas relacionadas a doenças genéticas específicas.\n",
    "\n",
    "### 3. Cargas em Batches:  \n",
    "Armazenar as notícias relevantes em um formato estruturado e facilmente acessível para consultas e análises posteriores. Essa carga deve acontecer 1 vez por hora. Se as notícias extraídas já tiverem sidos armazenadas na carga anterior, o processo deve ignorar e não armazenar as notícias novamente, os dados carregados não podem ficar duplicados.  \n",
    "![Alt text](../imagens/image.png)\n",
    "\n",
    "### 4. Dados transformados para consulta do público final  \n",
    "A partir dos dados carregados, aplicar as seguintes transformações e armazenar o resultado final para a consulta do público final:  \n",
    "\n",
    "4.1 - Quantidade de notícias por ano, mês e dia de publicação;  \n",
    "\n",
    "4.2 - Quantidade de notícias por fonte e autor;  \n",
    "\n",
    "4.3 - Quantidade de aparições de 3 palavras chaves por ano, mês e dia de publicação (as 3 palavras chaves serão as mesmas usadas para fazer os filtros de relevância do item 2 (2. Definir Critérios de Relevância)).  \n",
    "\n",
    "Atualizar os dados transformados 1 vez por dia.  \n",
    "\n",
    "![Alt text](../imagens/image-1.png)\n",
    "\n",
    "----------------------------------------\n",
    "\n",
    "Além das atividades principais, existe a necessidade de busca de dados por eventos em tempo real quando é necessário, para isso foi desenhado duas opções:\n",
    "\n",
    "### Opção 1 - Apache Kafka e Spark Streaming:  \n",
    "\n",
    "Preparar um pipeline com Apache Kafka e Spark Streaming para receber os dados do Produtor Kafka representado por um evento manual e consumir os dados com o Spark Streaming armazenando os resultados temporariamente. Em um processo paralelo, verificar os resultados armazenados temporiamente e armazenar no mesmo destino do item 3 (3. Cargas em Batches) aqueles resultados que ainda não foram armazenados no destino (os dados carregados não podem ficar duplicados). E por fim, eliminar os dados temporários após a verificação e a eventual carga.\n",
    "\n",
    "![Alt text](../imagens/image-2.png)\n",
    "\n",
    "### Opção 2 - Webhooks com notificações por eventos:  \n",
    "Configurar um webhook para adquirir as últimas notícias a partir de um evento representado por uma requisição POST e fazer a chamada da API e por fim armazenar os resultados temporariamente. Em um processo paralelo, verificar os resultados armazenados temporiamente e armazenar no mesmo destino do item 3 (3. Cargas em Batches) aqueles resultados que ainda não foram armazenados no destino (os dados carregados não podem ficar duplicados). E por fim, eliminar os dados temporários após a verificação e a eventual carga.\n",
    "\n",
    "![Alt text](../imagens/image-3.png)\n",
    "\n",
    "Atividades que precisam ser realizadas pelo grupo definido em aula.  \n",
    "\n",
    "O grupo precisa construir o pipeline de dados seguindo os requisitos das atividades principais e escolher entre a Opção 1 e Opção 2 para desenvolvimento.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Bibliotecas\n",
    "Para instalar as bibliotecas necessárias do projeto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\danie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: newsapi-python in c:\\users\\danie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.2.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\danie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests) (2.0.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\danie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\danie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\danie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Danie\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install requests newsapi-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para importas as bibliotecas e definir as variáveis necessárias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "\n",
    "API_KEY = '9a77398581d74beebbd29dbebd159a53'\n",
    "PALAVRAS_CHAVES_GERAIS = '(genômica OR genômico) AND (terapia OR sequenciamento OR doença)'\n",
    "PALAVRAS_CHAVES_ESPECIFICAS = ['terapia', 'sequenciamento', 'doença']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Definições de Funções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo está localizado todas as definições de funções que será usado no projeto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Consumo de dados com a News API -------------------------------------------------------------------------------------------------\n",
    "\n",
    "## 1 Usando Biblioteca NewsAPI\n",
    "def fazer_a_request_1( API_KEY:str='9a77398581d74beebbd29dbebd159a53' , PALAVRAS_CHAVES:str='(genômica OR genômico) AND (terapia OR sequenciamento OR doença)' ):\n",
    "    # A função realiza o request com API_KEY e PALAVRAS_CHAVES declaradas e é retornado um json\n",
    "    newsapi = NewsApiClient(api_key=API_KEY)\n",
    "    response_lib = newsapi.get_everything(q=PALAVRAS_CHAVES,\n",
    "                                        language='pt',\n",
    "                                        sort_by='publishedAt'\n",
    "    )\n",
    "    return response_lib\n",
    "\n",
    "## 2 Usando requests\n",
    "def fazer_a_request_2( API_KEY:str='9a77398581d74beebbd29dbebd159a53' , PALAVRAS_CHAVES:str='(genômica OR genômico) AND (terapia OR sequenciamento OR doença)' ):\n",
    "    # A função realiza o request com API_KEY e PALAVRAS_CHAVES declaradas e é retornado um json\n",
    "    url = f'https://newsapi.org/v2/everything?q={PALAVRAS_CHAVES}&language=pt&sortBy=publishedAt&apiKey={API_KEY}'\n",
    "    response_requests = requests.get(url).json()\n",
    "    return response_requests\n",
    "\n",
    "\n",
    "\n",
    "# 2. Definir Critérios de Relevância -------------------------------------------------------------------------------------------------\n",
    "\n",
    "def tratar_dados(json_de_noticias) -> pd.DataFrame:\n",
    "    df = pd.json_normalize(json_de_noticias['articles'])\n",
    "    df['publishedAt'] = pd.to_datetime(df['publishedAt']).dt.tz_localize(None) # inserido, pq linha abaixo estava dando erro \"TypeError: Cannot use .astype to convert from timezone-aware dtype to timezone-naive dtype\"\n",
    "    df['publishedAt'] = df['publishedAt'].astype('datetime64[ms]')\n",
    "    df.fillna(\"desconhecido\", inplace=True)\n",
    "\n",
    "    # Remover noticias sem informações nas colunas: descrição, conteúdo e nome de fonte \n",
    "    df = df[~df['description'].str.contains('desconhecido', case=False)]\n",
    "    df = df[~df['content'].str.contains('desconhecido', case=False)]\n",
    "    df = df[~df['source.name'].str.contains('desconhecido', case=False)]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# 3. Cargas em Batches: ---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def armazenar_noticias(df) -> None:\n",
    "\n",
    "    # Verifica se já existe um arquivo com os dados\n",
    "    if os.path.isfile('noticias.csv'):\n",
    "        # Lê o arquivo existente\n",
    "        df_existente = carrega_armazenamento()\n",
    "\n",
    "        # Identifica as notícias repetidas\n",
    "        df_repetidas = df[df['title'].isin(df_existente['title'])]\n",
    "\n",
    "        # Exclui as notícias repetidas\n",
    "        df = df.drop(df_repetidas.index)\n",
    "\n",
    "        # Grava o dataframe em um arquivo CSV\n",
    "        df.to_csv('noticias.csv', index=0, mode='a', header=False)\n",
    "    else:\n",
    "        df.to_csv('noticias.csv', index=0)\n",
    "\n",
    "    return None\n",
    "\n",
    "def carrega_armazenamento():\n",
    "    df = pd.read_csv('noticias.csv')\n",
    "    return df \n",
    "\n",
    "\n",
    "\n",
    "# 4. Dados transformados para consulta do público final ------------------------------------------------------------------------------\n",
    "\n",
    "def qtd_noticia_ano_mes_dia(df_inteiro):\n",
    "\n",
    "    # Calcula a quantidade de notícias por ano, mês/ano e dia/mês/ano\n",
    "    df_inteiro['ano'] = df_inteiro['publishedAt'].dt.year\n",
    "    df_inteiro['mes_ano'] = df_inteiro['publishedAt'].dt.to_period('M')\n",
    "    df_inteiro['dia_mes_ano'] = df_inteiro['publishedAt'].dt.to_period('D')\n",
    "    \n",
    "    qtd_por_ano = df_inteiro['ano'].value_counts().sort_index().reset_index()\n",
    "    qtd_por_ano.columns = ['ano', 'quantidade']\n",
    "\n",
    "    qtd_por_mes = df_inteiro['mes_ano'].value_counts().sort_index().reset_index()\n",
    "    qtd_por_mes.columns = ['mes_ano', 'quantidade']\n",
    "\n",
    "    qtd_por_dia = df_inteiro['dia_mes_ano'].value_counts().sort_index().reset_index()\n",
    "    qtd_por_dia.columns = ['dia_mes_ano', 'quantidade']\n",
    "    \n",
    "    return qtd_por_ano, qtd_por_mes, qtd_por_dia \n",
    "\n",
    "def qtd_noticia_fonte_autor(df_inteiro):\n",
    "    # Calcula a quantidade de notícias por fonte\n",
    "    qtd_por_fonte = df_inteiro['source.name'].value_counts().reset_index()\n",
    "    qtd_por_fonte.columns = ['fonte', 'quantidade']\n",
    "    \n",
    "    # Calcula a quantidade de notícias por autor\n",
    "    qtd_por_autor = df_inteiro['author'].value_counts().reset_index()\n",
    "    qtd_por_autor.columns = ['autor', 'quantidade']\n",
    "    \n",
    "    return qtd_por_fonte, qtd_por_autor\n",
    "\n",
    "def qtd_aparicao_palavras_chaves(df_inteiro, palavras_chave):  \n",
    "    # Adiciona colunas para cada palavra-chave no DataFrame\n",
    "    for palavra in palavras_chave:\n",
    "        df_inteiro[palavra] = df_inteiro['description'].str.contains(fr'\\b{palavra}\\b', case=False)\n",
    "    \n",
    "    df_inteiro['ano'] = df_inteiro['publishedAt'].dt.year\n",
    "    df_inteiro['mes'] = df_inteiro['publishedAt'].dt.month # Aproveitei aqui para deixar as colunas apenas do mês e do dia, pois serão útil na função \"qtd_aparicao_palavras_chaves\"\n",
    "    df_inteiro['dia'] = df_inteiro['publishedAt'].dt.day\n",
    "\n",
    "    # Agrupa por ano, mês e dia e conta a quantidade de aparições de cada palavra-chave\n",
    "    df_inteiro = df_inteiro.groupby(['ano', 'mes', 'dia'])[palavras_chave].sum().reset_index()\n",
    "    \n",
    "    # Cria um dataframe com as colunas 'ano', 'mes', 'dia' e a contagem de cada palavra-chave\n",
    "    df_contagem_palavras_chave = df_inteiro.melt(id_vars=['ano', 'mes', 'dia'], var_name='palavra_chave', value_name='quantidade')\n",
    "    \n",
    "    return df_contagem_palavras_chave   \n",
    "\n",
    "def armazenar_dados_estatisticos(qtd_por_ano, qtd_por_mes, qtd_por_dia, qtd_por_fonte, qtd_por_autor, df_contagem_palavras_chave):\n",
    "    print(\"Quantidade de notícias por ano:\")\n",
    "    print(qtd_por_ano)\n",
    "\n",
    "    print(\"\\nQuantidade de notícias por mês:\")\n",
    "    print(qtd_por_mes)\n",
    "\n",
    "    print(\"\\nQuantidade de notícias por dia:\")\n",
    "    print(qtd_por_dia)\n",
    "\n",
    "    print(\"\\nQuantidade de notícias por fonte:\")\n",
    "    print(qtd_por_fonte)\n",
    "\n",
    "    print(\"\\nQuantidade de notícias por autor:\")\n",
    "    print(qtd_por_autor)\n",
    "\n",
    "    print(\"\\nContagem de aparições das palavras-chave:\")\n",
    "    print(df_contagem_palavras_chave)\n",
    "    \n",
    "    estatisticas = pd.concat([qtd_por_ano, qtd_por_mes, qtd_por_dia, qtd_por_fonte, qtd_por_autor, df_contagem_palavras_chave], axis=1)\n",
    "    estatisticas.to_csv('resultado_final.csv', index = False)\n",
    "    #estatisticas.to_excel('resultado_final.xlsx', index = False)\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Descentralizando...\n",
    "\n",
    "def procedimento_hora_hora():\n",
    "    # 1. Consumo de dados com a News API\n",
    "    response = fazer_a_request_1(API_KEY, PALAVRAS_CHAVES_GERAIS)\n",
    "\n",
    "    # 2. Definir Critérios de Relevância\n",
    "    df_agora = tratar_dados(response)\n",
    "\n",
    "    # 3. Cargas em Batches:\n",
    "    armazenar_noticias(df_agora)\n",
    "    \n",
    "\n",
    "def procedimento_diario():\n",
    "    # 4. Dados transformados para consulta do público final\n",
    "    df_inteiro = carrega_armazenamento()\n",
    "    df_inteiro[\"publishedAt\"] = pd.to_datetime(df_inteiro[\"publishedAt\"])\n",
    "    a, b, c = qtd_noticia_ano_mes_dia(df_inteiro)\n",
    "    d, e = qtd_noticia_fonte_autor(df_inteiro)\n",
    "    f = qtd_aparicao_palavras_chaves(df_inteiro, PALAVRAS_CHAVES_ESPECIFICAS)\n",
    "    armazenar_dados_estatisticos(a , b , c, d, e, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chaves retornadas: dict_keys(['status', 'totalResults', 'articles'])\n",
      "Status: ok\n",
      "Total de resultados: 11\n",
      "Exemplo de artigo: {'source': {'id': None, 'name': 'Metropoles.com'}, 'author': 'Bethânia Nunes', 'title': 'IA: entenda como ela pode ser usada em benefício da saúde', 'description': 'A inteligência artificial (IA) já está presente na medicina no atendimento, tratamento e apoio ao paciente. Veja alguns exemplos de seu uso', 'url': 'https://www.metropoles.com/saude/inteligencia-artificial-uso-saude', 'urlToImage': 'https://uploads.metropoles.com/wp-content/uploads/2023/07/13112836/Mulher-ressonancia-cerebro-inteligencia-artificial.jpg', 'publishedAt': '2023-09-30T05:02:28Z', 'content': 'A inteligência artificial (IA) vem provocando uma grande transformação na medicina, e é provável que ela esteja em um dos consultórios que você frequenta. Embora o termo ainda pareça abstrato para gr… [+4477 chars]'}\n"
     ]
    }
   ],
   "source": [
    "# Instanciar objeto\n",
    "response = fazer_a_request_1()\n",
    "# Exibir chaves do dicionário\n",
    "print(f'Chaves retornadas: {response.keys()}')\n",
    "# Valores do dicionário\n",
    "print(f\"Status: {response['status']}\")\n",
    "print(f\"Total de resultados: {response['totalResults']}\")\n",
    "print(f\"Exemplo de artigo: {response['articles'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prova real que ambos os metodos de request dão o mesmo resultado\n",
    "print(fazer_a_request_1() == fazer_a_request_2())\n",
    "# teste do tratamento de dados\n",
    "df = tratar_dados(response)\n",
    "df[~df['author'].str.contains('desconhecido', case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 9 fields in line 3, saw 17\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\Google Drive\\CoisasMinhas\\ADA\\ADA-Santander-1010\\modulo 2\\ada_santander\\Modulo 4 - Extracao de dados I\\Projeto - Extração de Dados I\\src\\projeto.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Google%20Drive/CoisasMinhas/ADA/ADA-Santander-1010/modulo%202/ada_santander/Modulo%204%20-%20Extracao%20de%20dados%20I/Projeto%20-%20Extra%C3%A7%C3%A3o%20de%20Dados%20I/src/projeto.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mnoticias.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Google%20Drive/CoisasMinhas/ADA/ADA-Santander-1010/modulo%202/ada_santander/Modulo%204%20-%20Extracao%20de%20dados%20I/Projeto%20-%20Extra%C3%A7%C3%A3o%20de%20Dados%20I/src/projeto.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df \n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1771\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1773\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m     (\n\u001b[0;32m   1775\u001b[0m         index,\n\u001b[0;32m   1776\u001b[0m         columns,\n\u001b[0;32m   1777\u001b[0m         col_dict,\n\u001b[1;32m-> 1778\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1779\u001b[0m         nrows\n\u001b[0;32m   1780\u001b[0m     )\n\u001b[0;32m   1781\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 230\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    231\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    232\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 9 fields in line 3, saw 17\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('noticias.csv')\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carrega_armazenamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procedimento_hora_hora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Consumo de dados com a News API\n",
    "response = fazer_a_request_1(API_KEY, PALAVRAS_CHAVES_GERAIS)\n",
    "\n",
    "# 2. Definir Critérios de Relevância\n",
    "df_agora = tratar_dados(response)\n",
    "\n",
    "df_agora\n",
    "\n",
    "# 3. Cargas em Batches:\n",
    "# armazenar_noticias(df_agora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "import requests\n",
    "import time\n",
    "\n",
    "API_KEY = '9a77398581d74beebbd29dbebd159a53'\n",
    "PALAVRAS_CHAVES_GERAIS = '(genômica OR genômico) AND (terapia OR sequenciamento OR doença)'\n",
    "PALAVRAS_CHAVES_ESPECIFICAS = ['terapia', 'sequenciamento', 'doença']\n",
    "\n",
    "INTERVALO = 3.6 # 3600 segundos = 1 hora\n",
    "i = 20\n",
    "\n",
    "while True:\n",
    "\n",
    "    procedimento_hora_hora()\n",
    "    if i==23: # i=23 referente as 24h do dia\n",
    "        procedimento_diario()\n",
    "        i = 0\n",
    "        break   \n",
    "    else:\n",
    "        i += 1\n",
    "        \n",
    "    time.sleep(INTERVALO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "import requests\n",
    "import time\n",
    "\n",
    "API_KEY = '9a77398581d74beebbd29dbebd159a53'\n",
    "PALAVRAS_CHAVES_GERAIS = '(genômica OR genômico) AND (terapia OR sequenciamento OR doença)'\n",
    "PALAVRAS_CHAVES_ESPECIFICAS = ['terapia', 'sequenciamento', 'doença']\n",
    "\n",
    "INTERVALO = 3600 # 3600 segundos = 1 hora\n",
    "i = 0\n",
    "\n",
    "while True:\n",
    "\n",
    "    procedimento_hora_hora()\n",
    "    if i==23: # i=23 referente as 24h do dia\n",
    "        procedimento_diario()\n",
    "        i = 0       \n",
    "    else:\n",
    "        i += 1\n",
    "        \n",
    "    time.sleep(INTERVALO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

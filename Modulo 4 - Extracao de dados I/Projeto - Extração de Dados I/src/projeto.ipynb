{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enunciado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projeto - Extração de Dados I\n",
    "------------------------------\n",
    "## Sistema de Monitoramento de Avanços no Campo da Genômica  \n",
    "\n",
    "## Contexto:  \n",
    "O grupo trabalha no time de engenharia de dados na HealthGen, uma empresa especializada em genômica e pesquisa de medicina personalizada. A genômica é o estudo do conjunto completo de genes de um organismo, desempenha um papel fundamental na medicina personalizada e na pesquisa biomédica. Permite a análise do DNA para identificar variantes genéticas e mutações associadas a doenças e facilita a personalização de tratamentos com base nas características genéticas individuais dos pacientes.\n",
    "\n",
    "A empresa precisa se manter atualizada sobre os avanços mais recentes na genômica, identificar oportunidades para pesquisa e desenvolvimento de tratamentos personalizados e acompanhar as tendências em genômica que podem influenciar estratégias de pesquisa e desenvolvimento. Pensando nisso, o time de dados apresentou uma proposta de desenvolvimento de um sistema que coleta, analisa e apresenta as últimas notícias relacionadas à genômica e à medicina personalizada, e também estuda o avanço do campo nos últimos anos.\n",
    "\n",
    "O time de engenharia de dados tem como objetivo desenvolver e garantir um pipeline de dados confiável e estável. As principais atividades são:\n",
    "\n",
    "### 1. Consumo de dados com a News API:  \n",
    "Implementar um mecanismo para consumir dados de notícias de fontes confiáveis e especializadas em genômica e medicina personalizada, a partir da News API:  \n",
    "https://newsapi.org/\n",
    "\n",
    "### 2. Definir Critérios de Relevância:  \n",
    "Desenvolver critérios precisos de relevância para filtrar as notícias. Por exemplo, o time pode se concentrar em notícias que mencionem avanços em sequenciamento de DNA, terapias genéticas personalizadas ou descobertas relacionadas a doenças genéticas específicas.\n",
    "\n",
    "### 3. Cargas em Batches:  \n",
    "Armazenar as notícias relevantes em um formato estruturado e facilmente acessível para consultas e análises posteriores. Essa carga deve acontecer 1 vez por hora. Se as notícias extraídas já tiverem sidos armazenadas na carga anterior, o processo deve ignorar e não armazenar as notícias novamente, os dados carregados não podem ficar duplicados.  \n",
    "![Alt text](image.png)\n",
    "\n",
    "### 4. Dados transformados para consulta do público final  \n",
    "A partir dos dados carregados, aplicar as seguintes transformações e armazenar o resultado final para a consulta do público final:  \n",
    "\n",
    "4.1 - Quantidade de notícias por ano, mês e dia de publicação;  \n",
    "\n",
    "4.2 - Quantidade de notícias por fonte e autor;  \n",
    "\n",
    "4.3 - Quantidade de aparições de 3 palavras chaves por ano, mês e dia de publicação (as 3 palavras chaves serão as mesmas usadas para fazer os filtros de relevância do item 2 (2. Definir Critérios de Relevância)).  \n",
    "\n",
    "Atualizar os dados transformados 1 vez por dia.  \n",
    "\n",
    "![Alt text](image-1.png)\n",
    "\n",
    "----------------------------------------\n",
    "\n",
    "Além das atividades principais, existe a necessidade de busca de dados por eventos em tempo real quando é necessário, para isso foi desenhado duas opções:\n",
    "\n",
    "### Opção 1 - Apache Kafka e Spark Streaming:  \n",
    "\n",
    "Preparar um pipeline com Apache Kafka e Spark Streaming para receber os dados do Produtor Kafka representado por um evento manual e consumir os dados com o Spark Streaming armazenando os resultados temporariamente. Em um processo paralelo, verificar os resultados armazenados temporiamente e armazenar no mesmo destino do item 3 (3. Cargas em Batches) aqueles resultados que ainda não foram armazenados no destino (os dados carregados não podem ficar duplicados). E por fim, eliminar os dados temporários após a verificação e a eventual carga.\n",
    "\n",
    "![Alt text](image-2.png)\n",
    "\n",
    "### Opção 2 - Webhooks com notificações por eventos:  \n",
    "Configurar um webhook para adquirir as últimas notícias a partir de um evento representado por uma requisição POST e fazer a chamada da API e por fim armazenar os resultados temporariamente. Em um processo paralelo, verificar os resultados armazenados temporiamente e armazenar no mesmo destino do item 3 (3. Cargas em Batches) aqueles resultados que ainda não foram armazenados no destino (os dados carregados não podem ficar duplicados). E por fim, eliminar os dados temporários após a verificação e a eventual carga.\n",
    "\n",
    "![Alt text](image-3.png)\n",
    "\n",
    "Atividades que precisam ser realizadas pelo grupo definido em aula.  \n",
    "\n",
    "O grupo precisa construir o pipeline de dados seguindo os requisitos das atividades principais e escolher entre a Opção 1 e Opção 2 para desenvolvimento.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Bibliotecas\n",
    "Para instalar as bibliotecas necessárias do projeto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\arthu\\anaconda3\\lib\\site-packages (2.31.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: newsapi-python in c:\\users\\arthu\\anaconda3\\lib\\site-packages (0.2.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\arthu\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arthu\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arthu\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arthu\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "pip install requests newsapi-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para importas as bibliotecas e definir as variáveis necessárias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "\n",
    "API_KEY = '9a77398581d74beebbd29dbebd159a53'\n",
    "PALAVRAS_CHAVES_GERAIS = '(genômica OR genômico) AND (terapia OR sequenciamento OR doença)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Definições de Funções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo está localizado todas as definições de funções que será usado no projeto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Consumo de dados com a News API -------------------------------------------------------------------------------------------------\n",
    "\n",
    "## 1 Usando Biblioteca NewsAPI\n",
    "def fazer_a_request_1( API_KEY:str='9a77398581d74beebbd29dbebd159a53' , PALAVRAS_CHAVES:str='(genômica OR genômico) AND (terapia OR sequenciamento OR doença)' ):\n",
    "    # A função realiza o request com API_KEY e PALAVRAS_CHAVES declaradas e é retornado um json\n",
    "    newsapi = NewsApiClient(api_key=API_KEY)\n",
    "    response_lib = newsapi.get_everything(q=PALAVRAS_CHAVES,\n",
    "                                        language='pt',\n",
    "                                        sort_by='publishedAt'\n",
    "    )\n",
    "    return response_lib\n",
    "\n",
    "## 2 Usando requests\n",
    "def fazer_a_request_2( API_KEY:str='9a77398581d74beebbd29dbebd159a53' , PALAVRAS_CHAVES:str='(genômica OR genômico) AND (terapia OR sequenciamento OR doença)' ):\n",
    "    # A função realiza o request com API_KEY e PALAVRAS_CHAVES declaradas e é retornado um json\n",
    "    url = f'https://newsapi.org/v2/everything?q={PALAVRAS_CHAVES}&language=pt&sortBy=publishedAt&apiKey={API_KEY}'\n",
    "    response_requests = requests.get(url).json()\n",
    "    return response_requests\n",
    "\n",
    "\n",
    "\n",
    "# 2. Definir Critérios de Relevância -------------------------------------------------------------------------------------------------\n",
    "\n",
    "def tratar_dados(json_de_noticias) -> pd.DataFrame:\n",
    "    df = pd.json_normalize(json_de_noticias['articles'])\n",
    "    df['publishedAt'] = pd.to_datetime(df['publishedAt']).dt.tz_localize(None) # inserido, pq linha abaixo estava dando erro \"TypeError: Cannot use .astype to convert from timezone-aware dtype to timezone-naive dtype\"\n",
    "    df['publishedAt'] = df['publishedAt'].astype('datetime64[ms]')\n",
    "    df.fillna(\"desconhecido\", inplace=True)\n",
    "\n",
    "    # Remover noticias sem informações nas colunas: descrição, conteúdo e nome de fonte \n",
    "    df = df[~df['description'].str.contains('desconhecido', case=False)]\n",
    "    df = df[~df['content'].str.contains('desconhecido', case=False)]\n",
    "    df = df[~df['source.name'].str.contains('desconhecido', case=False)]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# 3. Cargas em Batches: ---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def armazenar_noticias(df) -> None:\n",
    "\n",
    "    # Verifica se já existe um arquivo com os dados\n",
    "    if os.path.isfile('noticias.csv'):\n",
    "        # Lê o arquivo existente\n",
    "        df_existente = pd.read_csv('noticias.csv', index_col=0)\n",
    "\n",
    "        # Identifica as notícias repetidas\n",
    "        df_repetidas = df[df['title'].isin(df_existente['title'])]\n",
    "\n",
    "        # Exclui as notícias repetidas\n",
    "        df = df.drop(df_repetidas.index)\n",
    "\n",
    "    # Grava o dataframe em um arquivo CSV\n",
    "    df.to_csv('noticias.csv', index=False)\n",
    "\n",
    "    return None\n",
    "\n",
    "def carrega_armazenamento():\n",
    "    df = pd.read_csv('noticias.csv')\n",
    "    return df \n",
    "\n",
    "\n",
    "\n",
    "# 4. Dados transformados para consulta do público final ------------------------------------------------------------------------------\n",
    "\n",
    "def qtd_noticia_ano_mes_dia(df_inteiro): \n",
    "# =============================================================================\n",
    "#     #Alternativa 1\n",
    "#     # Calcula a quantidade de notícias por ano, mês e dia\n",
    "#     df_inteiro['ano'] = df_inteiro['publishedAt'].dt.year\n",
    "#     df_inteiro['mes'] = df_inteiro['publishedAt'].dt.month\n",
    "#     df_inteiro['dia'] = df_inteiro['publishedAt'].dt.day\n",
    "# \n",
    "#     qtd_por_ano = df_inteiro['ano'].value_counts().sort_index().reset_index()\n",
    "#     qtd_por_ano.columns = ['ano', 'quantidade']\n",
    "# \n",
    "#     qtd_por_mes = df_inteiro['mes'].value_counts().sort_index().reset_index()\n",
    "#     qtd_por_mes.columns = ['mes', 'quantidade']\n",
    "# \n",
    "#     qtd_por_dia = df_inteiro['dia'].value_counts().sort_index().reset_index()\n",
    "#     qtd_por_dia.columns = ['dia', 'quantidade']\n",
    "# =============================================================================\n",
    "    \n",
    "    #Alternativa 2\n",
    "    # Calcula a quantidade de notícias por ano, mês/ano e dia/mês/ano\n",
    "    df_inteiro['ano'] = df_inteiro['publishedAt'].dt.year\n",
    "    df_inteiro['mes_ano'] = df_inteiro['publishedAt'].dt.to_period('M')\n",
    "    df_inteiro['dia_mes_ano'] = df_inteiro['publishedAt'].dt.to_period('D')\n",
    "    \n",
    "    qtd_por_ano = df_inteiro['ano'].value_counts().sort_index().reset_index()\n",
    "    qtd_por_ano.columns = ['ano', 'quantidade']\n",
    "\n",
    "    qtd_por_mes = df_inteiro['mes_ano'].value_counts().sort_index().reset_index()\n",
    "    qtd_por_mes.columns = ['mes_ano', 'quantidade']\n",
    "\n",
    "    qtd_por_dia = df_inteiro['dia_mes_ano'].value_counts().sort_index().reset_index()\n",
    "    qtd_por_dia.columns = ['dia_mes_ano', 'quantidade']\n",
    "    \n",
    "    return qtd_por_ano, qtd_por_mes, qtd_por_dia \n",
    "\n",
    "def qtd_noticia_fonte_autor(df_inteiro):\n",
    "    # Calcula a quantidade de notícias por fonte\n",
    "    qtd_por_fonte = df_inteiro['source.name'].value_counts().reset_index()\n",
    "    qtd_por_fonte.columns = ['fonte', 'quantidade']\n",
    "    \n",
    "    # Calcula a quantidade de notícias por autor\n",
    "    qtd_por_autor = df_inteiro['author'].value_counts().reset_index()\n",
    "    qtd_por_autor.columns = ['autor', 'quantidade']\n",
    "    \n",
    "    return qtd_por_fonte, qtd_por_autor\n",
    "\n",
    "def qtd_aparicao_palavras_chaves(df_inteiro, palavras_chave):  \n",
    "    # Adiciona colunas para cada palavra-chave no DataFrame\n",
    "    for palavra in palavras_chave:\n",
    "        df_inteiro[palavra] = df_inteiro['description'].str.contains(fr'\\b{palavra}\\b', case=False)\n",
    "    \n",
    "    df_inteiro['ano'] = df_inteiro['publishedAt'].dt.year\n",
    "    df_inteiro['mes'] = df_inteiro['publishedAt'].dt.month # Aproveitei aqui para deixar as colunas apenas do mês e do dia, pois serão útil na função \"qtd_aparicao_palavras_chaves\"\n",
    "    df_inteiro['dia'] = df_inteiro['publishedAt'].dt.day\n",
    "\n",
    "    # Agrupa por ano, mês e dia e conta a quantidade de aparições de cada palavra-chave\n",
    "    df_inteiro = df_inteiro.groupby(['ano', 'mes', 'dia'])[palavras_chave].sum().reset_index()\n",
    "    \n",
    "    # Cria um dataframe com as colunas 'ano', 'mes', 'dia' e a contagem de cada palavra-chave\n",
    "    df_contagem_palavras_chave = df_inteiro.melt(id_vars=['ano', 'mes', 'dia'], var_name='palavra_chave', value_name='quantidade')\n",
    "    \n",
    "    return df_contagem_palavras_chave   \n",
    "\n",
    "def armazenar_dados_estatisticos(qtd_por_ano, qtd_por_mes, qtd_por_dia, qtd_por_fonte, qtd_por_autor, df_contagem_palavras_chave):\n",
    "    print(\"Quantidade de notícias por ano:\")\n",
    "    print(qtd_por_ano)\n",
    "\n",
    "    print(\"\\nQuantidade de notícias por mês:\")\n",
    "    print(qtd_por_mes)\n",
    "\n",
    "    print(\"\\nQuantidade de notícias por dia:\")\n",
    "    print(qtd_por_dia)\n",
    "\n",
    "    print(\"\\nQuantidade de notícias por fonte:\")\n",
    "    print(qtd_por_fonte)\n",
    "\n",
    "    print(\"\\nQuantidade de notícias por autor:\")\n",
    "    print(qtd_por_autor)\n",
    "\n",
    "    print(\"\\nContagem de aparições das palavras-chave:\")\n",
    "    print(df_contagem_palavras_chave)\n",
    "    \n",
    "    estatisticas = pd.concat([qtd_por_ano, qtd_por_mes, qtd_por_dia, qtd_por_fonte, qtd_por_autor, df_contagem_palavras_chave], axis=1)\n",
    "    estatisticas.to_csv('resultado_final.csv', index = False)\n",
    "    estatisticas.to_excel('resultado_final.xlsx', index = False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chaves retornadas: dict_keys(['status', 'totalResults', 'articles'])\n",
      "Status: ok\n",
      "Total de resultados: 20\n",
      "Exemplo de artigo: {'source': {'id': None, 'name': 'Terra.com.br'}, 'author': 'Alma Preta', 'title': 'Nordeste é a região do país com maior ancestralidade da África e do Oriente Médio', 'description': 'Levantamento inédito traz dados sobre a ancestralidade dos brasileiros por estado, com base em um banco de dados de mais de 260 mil DNAs Texto: Redação | Foto: Reprodução', 'url': 'https://www.terra.com.br/nos/nordeste-e-a-regiao-do-pais-com-maior-ancestralidade-da-africa-e-do-oriente-medio,37ce6d627d490dd89afcd004cfede315rw30iu52.html', 'urlToImage': 'https://p2.trrsf.com/image/fget/cf/1200/630/middle/images.terra.com/2023/09/29/2012010853-brasilmapa.jpeg', 'publishedAt': '2023-09-29T13:13:29Z', 'content': 'Devido à sua história repleta de migrações, o Brasil é um dos países mais miscigenados do planeta. E isso se reflete na diversidade presente no DNA da população. As três ancestralidades que mais se d… [+8697 chars]'}\n"
     ]
    }
   ],
   "source": [
    "# Instanciar objeto\n",
    "response = fazer_a_request_1()\n",
    "# Exibir chaves do dicionário\n",
    "print(f'Chaves retornadas: {response.keys()}')\n",
    "# Valores do dicionário\n",
    "print(f\"Status: {response['status']}\")\n",
    "print(f\"Total de resultados: {response['totalResults']}\")\n",
    "print(f\"Exemplo de artigo: {response['articles'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>urlToImage</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>content</th>\n",
       "      <th>source.id</th>\n",
       "      <th>source.name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Redação</td>\n",
       "      <td>Este hábito comum pode duplicar o risco de doe...</td>\n",
       "      <td>Todos sabem que o hábito de fumar pode causar ...</td>\n",
       "      <td>https://catracalivre.com.br/saude-bem-estar/es...</td>\n",
       "      <td>https://catracalivre.com.br/cdn-cgi/image/f=au...</td>\n",
       "      <td>2023-09-13 04:00:01</td>\n",
       "      <td>Todos sabem que o hábito de fumar pode causar ...</td>\n",
       "      <td>desconhecido</td>\n",
       "      <td>Catracalivre.com.br</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>desconhecido</td>\n",
       "      <td>O que se sabe sobre a nova onda de covid no Br...</td>\n",
       "      <td>Alta de casos da doença pode estar relacionada...</td>\n",
       "      <td>https://valor.globo.com/brasil/noticia/2023/09...</td>\n",
       "      <td>https://s2-valor.glbimg.com/pOcRs2rEKVjvZxWDKI...</td>\n",
       "      <td>2023-09-01 19:07:09</td>\n",
       "      <td>Os testes positivos para covid-19 aumentaram n...</td>\n",
       "      <td>globo</td>\n",
       "      <td>Globo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author                                              title  \\\n",
       "11       Redação  Este hábito comum pode duplicar o risco de doe...   \n",
       "15  desconhecido  O que se sabe sobre a nova onda de covid no Br...   \n",
       "\n",
       "                                          description  \\\n",
       "11  Todos sabem que o hábito de fumar pode causar ...   \n",
       "15  Alta de casos da doença pode estar relacionada...   \n",
       "\n",
       "                                                  url  \\\n",
       "11  https://catracalivre.com.br/saude-bem-estar/es...   \n",
       "15  https://valor.globo.com/brasil/noticia/2023/09...   \n",
       "\n",
       "                                           urlToImage         publishedAt  \\\n",
       "11  https://catracalivre.com.br/cdn-cgi/image/f=au... 2023-09-13 04:00:01   \n",
       "15  https://s2-valor.glbimg.com/pOcRs2rEKVjvZxWDKI... 2023-09-01 19:07:09   \n",
       "\n",
       "                                              content     source.id  \\\n",
       "11  Todos sabem que o hábito de fumar pode causar ...  desconhecido   \n",
       "15  Os testes positivos para covid-19 aumentaram n...         globo   \n",
       "\n",
       "            source.name  \n",
       "11  Catracalivre.com.br  \n",
       "15                Globo  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prova real que ambos os metodos de request dão o mesmo resultado\n",
    "print(fazer_a_request_1() == fazer_a_request_2())\n",
    "df = tratar_dados(response)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de notícias por ano:\n",
      "    ano  quantidade\n",
      "0  2023           5\n",
      "\n",
      "Quantidade de notícias por mês:\n",
      "   mes_ano  quantidade\n",
      "0  2023-09           5\n",
      "\n",
      "Quantidade de notícias por dia:\n",
      "  dia_mes_ano  quantidade\n",
      "0  2023-09-27           1\n",
      "1  2023-09-28           1\n",
      "2  2023-09-29           3\n",
      "\n",
      "Quantidade de notícias por fonte:\n",
      "                 fonte  quantidade\n",
      "0               R7.com           2\n",
      "1         Terra.com.br           1\n",
      "2  Catracalivre.com.br           1\n",
      "3   Megacurioso.com.br           1\n",
      "\n",
      "Quantidade de notícias por autor:\n",
      "                autor  quantidade\n",
      "0  Redação Terra Você           1\n",
      "1               Do R7           1\n",
      "2   Da Agência Brasil           1\n",
      "3             Redação           1\n",
      "4       Pedro Freitas           1\n",
      "\n",
      "Contagem de aparições das palavras-chave:\n",
      "    ano  mes  dia   palavra_chave  quantidade\n",
      "0  2023    9   27  sequenciamento           0\n",
      "1  2023    9   28  sequenciamento           0\n",
      "2  2023    9   29  sequenciamento           0\n",
      "3  2023    9   27         terapia           0\n",
      "4  2023    9   28         terapia           0\n",
      "5  2023    9   29         terapia           0\n",
      "6  2023    9   27          doença           1\n",
      "7  2023    9   28          doença           1\n",
      "8  2023    9   29          doença           2\n",
      "Quantidade de notícias por ano:\n",
      "    ano  quantidade\n",
      "0  2023           5\n",
      "\n",
      "Quantidade de notícias por mês:\n",
      "   mes_ano  quantidade\n",
      "0  2023-09           5\n",
      "\n",
      "Quantidade de notícias por dia:\n",
      "  dia_mes_ano  quantidade\n",
      "0  2023-09-27           1\n",
      "1  2023-09-28           1\n",
      "2  2023-09-29           3\n",
      "\n",
      "Quantidade de notícias por fonte:\n",
      "                 fonte  quantidade\n",
      "0               R7.com           2\n",
      "1         Terra.com.br           1\n",
      "2  Catracalivre.com.br           1\n",
      "3   Megacurioso.com.br           1\n",
      "\n",
      "Quantidade de notícias por autor:\n",
      "                autor  quantidade\n",
      "0  Redação Terra Você           1\n",
      "1               Do R7           1\n",
      "2   Da Agência Brasil           1\n",
      "3             Redação           1\n",
      "4       Pedro Freitas           1\n",
      "\n",
      "Contagem de aparições das palavras-chave:\n",
      "    ano  mes  dia   palavra_chave  quantidade\n",
      "0  2023    9   27  sequenciamento           0\n",
      "1  2023    9   28  sequenciamento           0\n",
      "2  2023    9   29  sequenciamento           0\n",
      "3  2023    9   27         terapia           0\n",
      "4  2023    9   28         terapia           0\n",
      "5  2023    9   29         terapia           0\n",
      "6  2023    9   27          doença           1\n",
      "7  2023    9   28          doença           1\n",
      "8  2023    9   29          doença           2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\arthu\\OneDrive\\Documents\\Estudos\\ADA Tech\\Módulo 4 - Projeto - API News\\ada_santander\\Modulo 4 - Extracao de dados I\\Projeto - Extração de Dados I\\src\\projeto.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/arthu/OneDrive/Documents/Estudos/ADA%20Tech/M%C3%B3dulo%204%20-%20Projeto%20-%20API%20News/ada_santander/Modulo%204%20-%20Extracao%20de%20dados%20I/Projeto%20-%20Extra%C3%A7%C3%A3o%20de%20Dados%20I/src/projeto.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/arthu/OneDrive/Documents/Estudos/ADA%20Tech/M%C3%B3dulo%204%20-%20Projeto%20-%20API%20News/ada_santander/Modulo%204%20-%20Extracao%20de%20dados%20I/Projeto%20-%20Extra%C3%A7%C3%A3o%20de%20Dados%20I/src/projeto.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arthu/OneDrive/Documents/Estudos/ADA%20Tech/M%C3%B3dulo%204%20-%20Projeto%20-%20API%20News/ada_santander/Modulo%204%20-%20Extracao%20de%20dados%20I/Projeto%20-%20Extra%C3%A7%C3%A3o%20de%20Dados%20I/src/projeto.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arthu/OneDrive/Documents/Estudos/ADA%20Tech/M%C3%B3dulo%204%20-%20Projeto%20-%20API%20News/ada_santander/Modulo%204%20-%20Extracao%20de%20dados%20I/Projeto%20-%20Extra%C3%A7%C3%A3o%20de%20Dados%20I/src/projeto.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# 1. Consumo de dados com a News API\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/arthu/OneDrive/Documents/Estudos/ADA%20Tech/M%C3%B3dulo%204%20-%20Projeto%20-%20API%20News/ada_santander/Modulo%204%20-%20Extracao%20de%20dados%20I/Projeto%20-%20Extra%C3%A7%C3%A3o%20de%20Dados%20I/src/projeto.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     response \u001b[39m=\u001b[39m fazer_a_request_1(API_KEY, PALAVRAS_CHAVES_GERAIS)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arthu/OneDrive/Documents/Estudos/ADA%20Tech/M%C3%B3dulo%204%20-%20Projeto%20-%20API%20News/ada_santander/Modulo%204%20-%20Extracao%20de%20dados%20I/Projeto%20-%20Extra%C3%A7%C3%A3o%20de%20Dados%20I/src/projeto.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# 2. Definir Critérios de Relevância\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arthu/OneDrive/Documents/Estudos/ADA%20Tech/M%C3%B3dulo%204%20-%20Projeto%20-%20API%20News/ada_santander/Modulo%204%20-%20Extracao%20de%20dados%20I/Projeto%20-%20Extra%C3%A7%C3%A3o%20de%20Dados%20I/src/projeto.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     PALAVRAS_CHAVES_ESPECIFICAS \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39msequenciamento\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mterapia\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdoença\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;32mc:\\Users\\arthu\\OneDrive\\Documents\\Estudos\\ADA Tech\\Módulo 4 - Projeto - API News\\ada_santander\\Modulo 4 - Extracao de dados I\\Projeto - Extração de Dados I\\src\\projeto.ipynb Cell 15\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/arthu/OneDrive/Documents/Estudos/ADA%20Tech/M%C3%B3dulo%204%20-%20Projeto%20-%20API%20News/ada_santander/Modulo%204%20-%20Extracao%20de%20dados%20I/Projeto%20-%20Extra%C3%A7%C3%A3o%20de%20Dados%20I/src/projeto.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfazer_a_request_1\u001b[39m( API_KEY:\u001b[39mstr\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m9a77398581d74beebbd29dbebd159a53\u001b[39m\u001b[39m'\u001b[39m , PALAVRAS_CHAVES:\u001b[39mstr\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgenômica\u001b[39m\u001b[39m'\u001b[39m ):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/arthu/OneDrive/Documents/Estudos/ADA%20Tech/M%C3%B3dulo%204%20-%20Projeto%20-%20API%20News/ada_santander/Modulo%204%20-%20Extracao%20de%20dados%20I/Projeto%20-%20Extra%C3%A7%C3%A3o%20de%20Dados%20I/src/projeto.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# A função realiza o request com API_KEY e PALAVRAS_CHAVES declaradas e é retornado um json\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/arthu/OneDrive/Documents/Estudos/ADA%20Tech/M%C3%B3dulo%204%20-%20Projeto%20-%20API%20News/ada_santander/Modulo%204%20-%20Extracao%20de%20dados%20I/Projeto%20-%20Extra%C3%A7%C3%A3o%20de%20Dados%20I/src/projeto.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     newsapi \u001b[39m=\u001b[39m NewsApiClient(api_key\u001b[39m=\u001b[39mAPI_KEY)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/arthu/OneDrive/Documents/Estudos/ADA%20Tech/M%C3%B3dulo%204%20-%20Projeto%20-%20API%20News/ada_santander/Modulo%204%20-%20Extracao%20de%20dados%20I/Projeto%20-%20Extra%C3%A7%C3%A3o%20de%20Dados%20I/src/projeto.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     response_lib \u001b[39m=\u001b[39m newsapi\u001b[39m.\u001b[39mget_everything(q\u001b[39m=\u001b[39mPALAVRAS_CHAVES,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/arthu/OneDrive/Documents/Estudos/ADA%20Tech/M%C3%B3dulo%204%20-%20Projeto%20-%20API%20News/ada_santander/Modulo%204%20-%20Extracao%20de%20dados%20I/Projeto%20-%20Extra%C3%A7%C3%A3o%20de%20Dados%20I/src/projeto.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                                         language\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/arthu/OneDrive/Documents/Estudos/ADA%20Tech/M%C3%B3dulo%204%20-%20Projeto%20-%20API%20News/ada_santander/Modulo%204%20-%20Extracao%20de%20dados%20I/Projeto%20-%20Extra%C3%A7%C3%A3o%20de%20Dados%20I/src/projeto.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                                         sort_by\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpublishedAt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arthu/OneDrive/Documents/Estudos/ADA%20Tech/M%C3%B3dulo%204%20-%20Projeto%20-%20API%20News/ada_santander/Modulo%204%20-%20Extracao%20de%20dados%20I/Projeto%20-%20Extra%C3%A7%C3%A3o%20de%20Dados%20I/src/projeto.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arthu/OneDrive/Documents/Estudos/ADA%20Tech/M%C3%B3dulo%204%20-%20Projeto%20-%20API%20News/ada_santander/Modulo%204%20-%20Extracao%20de%20dados%20I/Projeto%20-%20Extra%C3%A7%C3%A3o%20de%20Dados%20I/src/projeto.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m response_lib\n",
      "File \u001b[1;32mc:\\Users\\arthu\\anaconda3\\Lib\\site-packages\\newsapi\\newsapi_client.py:330\u001b[0m, in \u001b[0;36mNewsApiClient.get_everything\u001b[1;34m(self, q, qintitle, sources, domains, exclude_domains, from_param, to, language, sort_by, page, page_size)\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mpage param should be an int\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    329\u001b[0m \u001b[39m# Send Request\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m r \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_method\u001b[39m.\u001b[39mget(const\u001b[39m.\u001b[39mEVERYTHING_URL, auth\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauth, timeout\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m, params\u001b[39m=\u001b[39mpayload)\n\u001b[0;32m    332\u001b[0m \u001b[39m# Check Status of Request\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[39mif\u001b[39;00m r\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m requests\u001b[39m.\u001b[39mcodes\u001b[39m.\u001b[39mok:\n",
      "File \u001b[1;32mc:\\Users\\arthu\\anaconda3\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39m\u001b[39mget\u001b[39m\u001b[39m\"\u001b[39m, url, params\u001b[39m=\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\arthu\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\arthu\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\arthu\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\arthu\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[39m=\u001b[39murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\arthu\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(\n\u001b[0;32m    715\u001b[0m     conn,\n\u001b[0;32m    716\u001b[0m     method,\n\u001b[0;32m    717\u001b[0m     url,\n\u001b[0;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout_obj,\n\u001b[0;32m    719\u001b[0m     body\u001b[39m=\u001b[39mbody,\n\u001b[0;32m    720\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39mchunked,\n\u001b[0;32m    722\u001b[0m )\n\u001b[0;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\arthu\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    461\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[0;32m    462\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m             six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    467\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    468\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\arthu\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    459\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[0;32m    460\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 461\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[0;32m    462\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\arthu\\anaconda3\\Lib\\http\\client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m         response\u001b[39m.\u001b[39mbegin()\n\u001b[0;32m   1379\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1380\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\arthu\\anaconda3\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\arthu\\anaconda3\\Lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline(_MAXLINE \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\arthu\\anaconda3\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sock\u001b[39m.\u001b[39mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\arthu\\anaconda3\\Lib\\ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\arthu\\anaconda3\\Lib\\ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m, buffer)\n\u001b[0;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from newsapi import NewsApiClient\n",
    "import requests\n",
    "import time\n",
    "\n",
    "API_KEY = '9a77398581d74beebbd29dbebd159a53'\n",
    "PALAVRAS_CHAVES_GERAIS = '(genômica OR genômico) AND (terapia OR sequenciamento OR doença)'\n",
    "INTERVALO = 3600 # 3600 segundos\n",
    "i = 0\n",
    "while True:\n",
    "\n",
    "    # 1. Consumo de dados com a News API\n",
    "    response = fazer_a_request_1(API_KEY, PALAVRAS_CHAVES_GERAIS)\n",
    "\n",
    "    # 2. Definir Critérios de Relevância\n",
    "    df_agora = tratar_dados(response)\n",
    "\n",
    "    # 3. Cargas em Batches:\n",
    "    armazenar_noticias(df_agora) # TODO\n",
    "    \n",
    "    # 4. Dados transformados para consulta do público final\n",
    "    if i==23: # i=23 referente as 24h do dia\n",
    "        df_inteiro = carrega_armazenamento() # TODO\n",
    "        df_inteiro[\"publishedAt\"] = pd.to_datetime(df_inteiro[\"publishedAt\"])\n",
    "        a, b, c = qtd_noticia_ano_mes_dia(df_inteiro) # TODO\n",
    "        d, e = qtd_noticia_fonte_autor(df_inteiro) # TODO\n",
    "        f = qtd_aparicao_palavras_chaves(df_inteiro, PALAVRAS_CHAVES_ESPECIFICAS) # TODO\n",
    "        armazenar_dados_estatisticos(a , b , c, d, e, f) # TODO\n",
    "        i = 0\n",
    "        \n",
    "    else:\n",
    "        i += 1\n",
    "        \n",
    "    time.sleep(INTERVALO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

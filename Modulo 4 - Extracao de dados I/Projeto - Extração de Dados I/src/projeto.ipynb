{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enunciado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projeto - Extração de Dados I\n",
    "------------------------------\n",
    "## Sistema de Monitoramento de Avanços no Campo da Genômica  \n",
    "\n",
    "## Contexto:  \n",
    "O grupo trabalha no time de engenharia de dados na HealthGen, uma empresa especializada em genômica e pesquisa de medicina personalizada. A genômica é o estudo do conjunto completo de genes de um organismo, desempenha um papel fundamental na medicina personalizada e na pesquisa biomédica. Permite a análise do DNA para identificar variantes genéticas e mutações associadas a doenças e facilita a personalização de tratamentos com base nas características genéticas individuais dos pacientes.\n",
    "\n",
    "A empresa precisa se manter atualizada sobre os avanços mais recentes na genômica, identificar oportunidades para pesquisa e desenvolvimento de tratamentos personalizados e acompanhar as tendências em genômica que podem influenciar estratégias de pesquisa e desenvolvimento. Pensando nisso, o time de dados apresentou uma proposta de desenvolvimento de um sistema que coleta, analisa e apresenta as últimas notícias relacionadas à genômica e à medicina personalizada, e também estuda o avanço do campo nos últimos anos.\n",
    "\n",
    "O time de engenharia de dados tem como objetivo desenvolver e garantir um pipeline de dados confiável e estável. As principais atividades são:\n",
    "\n",
    "### 1. Consumo de dados com a News API:  \n",
    "Implementar um mecanismo para consumir dados de notícias de fontes confiáveis e especializadas em genômica e medicina personalizada, a partir da News API:  \n",
    "https://newsapi.org/\n",
    "\n",
    "### 2. Definir Critérios de Relevância:  \n",
    "Desenvolver critérios precisos de relevância para filtrar as notícias. Por exemplo, o time pode se concentrar em notícias que mencionem avanços em sequenciamento de DNA, terapias genéticas personalizadas ou descobertas relacionadas a doenças genéticas específicas.\n",
    "\n",
    "### 3. Cargas em Batches:  \n",
    "Armazenar as notícias relevantes em um formato estruturado e facilmente acessível para consultas e análises posteriores. Essa carga deve acontecer 1 vez por hora. Se as notícias extraídas já tiverem sidos armazenadas na carga anterior, o processo deve ignorar e não armazenar as notícias novamente, os dados carregados não podem ficar duplicados.  \n",
    "![Alt text](image.png)\n",
    "\n",
    "### 4. Dados transformados para consulta do público final  \n",
    "A partir dos dados carregados, aplicar as seguintes transformações e armazenar o resultado final para a consulta do público final:  \n",
    "\n",
    "4.1 - Quantidade de notícias por ano, mês e dia de publicação;  \n",
    "\n",
    "4.2 - Quantidade de notícias por fonte e autor;  \n",
    "\n",
    "4.3 - Quantidade de aparições de 3 palavras chaves por ano, mês e dia de publicação (as 3 palavras chaves serão as mesmas usadas para fazer os filtros de relevância do item 2 (2. Definir Critérios de Relevância)).  \n",
    "\n",
    "Atualizar os dados transformados 1 vez por dia.  \n",
    "\n",
    "![Alt text](image-1.png)\n",
    "\n",
    "----------------------------------------\n",
    "\n",
    "Além das atividades principais, existe a necessidade de busca de dados por eventos em tempo real quando é necessário, para isso foi desenhado duas opções:\n",
    "\n",
    "### Opção 1 - Apache Kafka e Spark Streaming:  \n",
    "\n",
    "Preparar um pipeline com Apache Kafka e Spark Streaming para receber os dados do Produtor Kafka representado por um evento manual e consumir os dados com o Spark Streaming armazenando os resultados temporariamente. Em um processo paralelo, verificar os resultados armazenados temporiamente e armazenar no mesmo destino do item 3 (3. Cargas em Batches) aqueles resultados que ainda não foram armazenados no destino (os dados carregados não podem ficar duplicados). E por fim, eliminar os dados temporários após a verificação e a eventual carga.\n",
    "\n",
    "![Alt text](image-2.png)\n",
    "\n",
    "### Opção 2 - Webhooks com notificações por eventos:  \n",
    "Configurar um webhook para adquirir as últimas notícias a partir de um evento representado por uma requisição POST e fazer a chamada da API e por fim armazenar os resultados temporariamente. Em um processo paralelo, verificar os resultados armazenados temporiamente e armazenar no mesmo destino do item 3 (3. Cargas em Batches) aqueles resultados que ainda não foram armazenados no destino (os dados carregados não podem ficar duplicados). E por fim, eliminar os dados temporários após a verificação e a eventual carga.\n",
    "\n",
    "![Alt text](image-3.png)\n",
    "\n",
    "Atividades que precisam ser realizadas pelo grupo definido em aula.  \n",
    "\n",
    "O grupo precisa construir o pipeline de dados seguindo os requisitos das atividades principais e escolher entre a Opção 1 e Opção 2 para desenvolvimento.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Bibliotecas\n",
    "Para instalar as bibliotecas necessárias do projeto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install requests newsapi-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para importas as bibliotecas e definir as variáveis necessárias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "import requests\n",
    "import time\n",
    "\n",
    "API_KEY = '9a77398581d74beebbd29dbebd159a53'\n",
    "PALAVRAS_CHAVES_GERAIS = 'genômica OR genômico OR genética OR dna'\n",
    "PALAVRAS_CHAVES_ESPECIFICAS = \"sequenciamento\", \"terapia\", \"doença\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Definições de Funções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo está localizado todas as definições de funções que será usado no projeto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Consumo de dados com a News API -------------------------------------------------------------------------------------------------\n",
    "\n",
    "## 1 Usando Biblioteca NewsAPI\n",
    "def fazer_a_request_1( API_KEY:str='9a77398581d74beebbd29dbebd159a53' , PALAVRAS_CHAVES:str='genômica' ):\n",
    "    # A função realiza o request com API_KEY e PALAVRAS_CHAVES declaradas e é retornado um json\n",
    "    newsapi = NewsApiClient(api_key=API_KEY)\n",
    "    response_lib = newsapi.get_everything(q=PALAVRAS_CHAVES,\n",
    "                                        language='pt',\n",
    "                                        sort_by='publishedAt'\n",
    "    )\n",
    "    return response_lib\n",
    "\n",
    "## 2 Usando requests\n",
    "def fazer_a_request_2( API_KEY:str='9a77398581d74beebbd29dbebd159a53' , PALAVRAS_CHAVES:str='genômica' ):\n",
    "    # A função realiza o request com API_KEY e PALAVRAS_CHAVES declaradas e é retornado um json\n",
    "    url = f'https://newsapi.org/v2/everything?q={PALAVRAS_CHAVES}&language=pt&sortBy=publishedAt&apiKey={API_KEY}'\n",
    "    response_requests = requests.get(url).json()\n",
    "    return response_requests\n",
    "\n",
    "\n",
    "\n",
    "# 2. Definir Critérios de Relevância -------------------------------------------------------------------------------------------------\n",
    "\n",
    "def tratar_dados(json_de_noticias, *palavras_chaves):\n",
    "    # Recebe a resposa json, trata o dados retornando em uma dataframe\n",
    "\n",
    "    # OBS: a palavras_chaves vem em tupla\n",
    "    # print(palavras_chaves)\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "# 3. Cargas em Batches: ---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def armazenar_noticias(df) -> None:\n",
    "    # Recebe o dataframe para ser armazenado, verificar se há alguma notifica repetida e descarta, por fim armazena.\n",
    "    pass\n",
    "\n",
    "def carrega_armazenamento():\n",
    "    # Abre o armazenamentos dos dados coletados e os retonar em dataframe\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "# 4. Dados transformados para consulta do público final ------------------------------------------------------------------------------\n",
    "\n",
    "def qtd_noticia_ano_mes_dia(df_inteiro):\n",
    "    pass\n",
    "\n",
    "def qtd_noticia_fonte_autor(df_inteiro):\n",
    "    pass\n",
    "\n",
    "def qtd_aparicao_palavras_chaves(df_inteiro):\n",
    "    pass\n",
    "\n",
    "def armazenar_dados_estatisticos(A,B,C):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Usando Biblioteca NewsAPI\n",
    "\n",
    "def fazer_a_request_1() -> dict:\n",
    "    API_KEY = '9a77398581d74beebbd29dbebd159a53'\n",
    "    newsapi = NewsApiClient(api_key=API_KEY)\n",
    "\n",
    "    response_lib = newsapi.get_everything(q='genômica OR genômico',\n",
    "                                        language='pt',\n",
    "                                        sort_by='publishedAt'\n",
    "    )\n",
    "    return response_lib\n",
    "\n",
    "# 2 Usando requests\n",
    "def fazer_a_request_2() -> dict:\n",
    "    API_KEY = '9a77398581d74beebbd29dbebd159a53'\n",
    "    url = f'https://newsapi.org/v2/everything?q=genômica OR genômico&language=pt&sortBy=publishedAt&apiKey={API_KEY}'\n",
    "\n",
    "    response_requests = requests.get(url).json()\n",
    "    return response_requests\n",
    "\n",
    "response = response_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prova real que ambos os metodos de request dão o mesmo resultado\n",
    "\n",
    "fazer_a_request_1() == fazer_a_request_2()\n",
    "\n",
    "response = fazer_a_request_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para visualizar os resultados\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('sequenciamento', 'terapia', 'doença'),)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "import requests\n",
    "import time\n",
    "\n",
    "API_KEY = '9a77398581d74beebbd29dbebd159a53'\n",
    "PALAVRAS_CHAVES_GERAIS = 'genômica OR genômico OR genética OR dna'\n",
    "\n",
    "while True:\n",
    "    i = 0\n",
    "\n",
    "    # 1. Consumo de dados com a News API\n",
    "    response = fazer_a_request_1(API_KEY, PALAVRAS_CHAVES_GERAIS)\n",
    "\n",
    "    # 2. Definir Critérios de Relevância\n",
    "    PALAVRAS_CHAVES_ESPECIFICAS = \"sequenciamento\", \"terapia\", \"doença\"\n",
    "    df_agora = tratar_dados( response , PALAVRAS_CHAVES_ESPECIFICAS ) # TODO\n",
    "\n",
    "    # 3. Cargas em Batches:\n",
    "    armazenar_noticias( df_agora ) # TODO\n",
    "    \n",
    "    # 4. Dados transformados para consulta do público final\n",
    "    if i==23:\n",
    "        df_inteiro = carrega_armazenamento() # TODO\n",
    "        a = qtd_noticia_ano_mes_dia( df_inteiro ) # TODO\n",
    "        b = qtd_noticia_fonte_autor( df_inteiro ) # TODO\n",
    "        c = qtd_aparicao_palavras_chaves( df_inteiro ) # TODO\n",
    "        armazenar_dados_estatisticos( a , b , c ) # TODO\n",
    "        i = 0\n",
    "    else:\n",
    "        i += 1\n",
    "    time.sleep(3600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

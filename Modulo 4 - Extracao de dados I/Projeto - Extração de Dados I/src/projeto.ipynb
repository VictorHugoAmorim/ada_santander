{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enunciado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projeto - Extração de Dados I\n",
    "------------------------------\n",
    "## Sistema de Monitoramento de Avanços no Campo da Genômica  \n",
    "\n",
    "## Contexto:  \n",
    "O grupo trabalha no time de engenharia de dados na HealthGen, uma empresa especializada em genômica e pesquisa de medicina personalizada. A genômica é o estudo do conjunto completo de genes de um organismo, desempenha um papel fundamental na medicina personalizada e na pesquisa biomédica. Permite a análise do DNA para identificar variantes genéticas e mutações associadas a doenças e facilita a personalização de tratamentos com base nas características genéticas individuais dos pacientes.\n",
    "\n",
    "A empresa precisa se manter atualizada sobre os avanços mais recentes na genômica, identificar oportunidades para pesquisa e desenvolvimento de tratamentos personalizados e acompanhar as tendências em genômica que podem influenciar estratégias de pesquisa e desenvolvimento. Pensando nisso, o time de dados apresentou uma proposta de desenvolvimento de um sistema que coleta, analisa e apresenta as últimas notícias relacionadas à genômica e à medicina personalizada, e também estuda o avanço do campo nos últimos anos.\n",
    "\n",
    "O time de engenharia de dados tem como objetivo desenvolver e garantir um pipeline de dados confiável e estável. As principais atividades são:\n",
    "\n",
    "### 1. Consumo de dados com a News API:  \n",
    "Implementar um mecanismo para consumir dados de notícias de fontes confiáveis e especializadas em genômica e medicina personalizada, a partir da News API:  \n",
    "https://newsapi.org/\n",
    "\n",
    "### 2. Definir Critérios de Relevância:  \n",
    "Desenvolver critérios precisos de relevância para filtrar as notícias. Por exemplo, o time pode se concentrar em notícias que mencionem avanços em sequenciamento de DNA, terapias genéticas personalizadas ou descobertas relacionadas a doenças genéticas específicas.\n",
    "\n",
    "### 3. Cargas em Batches:  \n",
    "Armazenar as notícias relevantes em um formato estruturado e facilmente acessível para consultas e análises posteriores. Essa carga deve acontecer 1 vez por hora. Se as notícias extraídas já tiverem sidos armazenadas na carga anterior, o processo deve ignorar e não armazenar as notícias novamente, os dados carregados não podem ficar duplicados.  \n",
    "![Alt text](image.png)\n",
    "\n",
    "### 4. Dados transformados para consulta do público final  \n",
    "A partir dos dados carregados, aplicar as seguintes transformações e armazenar o resultado final para a consulta do público final:  \n",
    "\n",
    "4.1 - Quantidade de notícias por ano, mês e dia de publicação;  \n",
    "\n",
    "4.2 - Quantidade de notícias por fonte e autor;  \n",
    "\n",
    "4.3 - Quantidade de aparições de 3 palavras chaves por ano, mês e dia de publicação (as 3 palavras chaves serão as mesmas usadas para fazer os filtros de relevância do item 2 (2. Definir Critérios de Relevância)).  \n",
    "\n",
    "Atualizar os dados transformados 1 vez por dia.  \n",
    "\n",
    "![Alt text](image-1.png)\n",
    "\n",
    "----------------------------------------\n",
    "\n",
    "Além das atividades principais, existe a necessidade de busca de dados por eventos em tempo real quando é necessário, para isso foi desenhado duas opções:\n",
    "\n",
    "### Opção 1 - Apache Kafka e Spark Streaming:  \n",
    "\n",
    "Preparar um pipeline com Apache Kafka e Spark Streaming para receber os dados do Produtor Kafka representado por um evento manual e consumir os dados com o Spark Streaming armazenando os resultados temporariamente. Em um processo paralelo, verificar os resultados armazenados temporiamente e armazenar no mesmo destino do item 3 (3. Cargas em Batches) aqueles resultados que ainda não foram armazenados no destino (os dados carregados não podem ficar duplicados). E por fim, eliminar os dados temporários após a verificação e a eventual carga.\n",
    "\n",
    "![Alt text](image-2.png)\n",
    "\n",
    "### Opção 2 - Webhooks com notificações por eventos:  \n",
    "Configurar um webhook para adquirir as últimas notícias a partir de um evento representado por uma requisição POST e fazer a chamada da API e por fim armazenar os resultados temporariamente. Em um processo paralelo, verificar os resultados armazenados temporiamente e armazenar no mesmo destino do item 3 (3. Cargas em Batches) aqueles resultados que ainda não foram armazenados no destino (os dados carregados não podem ficar duplicados). E por fim, eliminar os dados temporários após a verificação e a eventual carga.\n",
    "\n",
    "![Alt text](image-3.png)\n",
    "\n",
    "Atividades que precisam ser realizadas pelo grupo definido em aula.  \n",
    "\n",
    "O grupo precisa construir o pipeline de dados seguindo os requisitos das atividades principais e escolher entre a Opção 1 e Opção 2 para desenvolvimento.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Bibliotecas\n",
    "Para instalar as bibliotecas necessárias do projeto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\danie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: newsapi-python in c:\\users\\danie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.2.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\danie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests) (2.0.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\danie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\danie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\danie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Danie\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install requests newsapi-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para importas as bibliotecas e definir as variáveis necessárias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "\n",
    "API_KEY = '9a77398581d74beebbd29dbebd159a53'\n",
    "PALAVRAS_CHAVES_GERAIS = '(genômica OR genômico) AND (terapia OR sequenciamento OR doença)'\n",
    "PALAVRAS_CHAVES_ESPECIFICAS = ['terapia', 'sequenciamento', 'doença']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Definições de Funções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo está localizado todas as definições de funções que será usado no projeto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Consumo de dados com a News API -------------------------------------------------------------------------------------------------\n",
    "\n",
    "## 1 Usando Biblioteca NewsAPI\n",
    "def fazer_a_request_1( API_KEY:str='9a77398581d74beebbd29dbebd159a53' , PALAVRAS_CHAVES:str='(genômica OR genômico) AND (terapia OR sequenciamento OR doença)' ):\n",
    "    # A função realiza o request com API_KEY e PALAVRAS_CHAVES declaradas e é retornado um json\n",
    "    newsapi = NewsApiClient(api_key=API_KEY)\n",
    "    response_lib = newsapi.get_everything(q=PALAVRAS_CHAVES,\n",
    "                                        language='pt',\n",
    "                                        sort_by='publishedAt'\n",
    "    )\n",
    "    return response_lib\n",
    "\n",
    "## 2 Usando requests\n",
    "def fazer_a_request_2( API_KEY:str='9a77398581d74beebbd29dbebd159a53' , PALAVRAS_CHAVES:str='(genômica OR genômico) AND (terapia OR sequenciamento OR doença)' ):\n",
    "    # A função realiza o request com API_KEY e PALAVRAS_CHAVES declaradas e é retornado um json\n",
    "    url = f'https://newsapi.org/v2/everything?q={PALAVRAS_CHAVES}&language=pt&sortBy=publishedAt&apiKey={API_KEY}'\n",
    "    response_requests = requests.get(url).json()\n",
    "    return response_requests\n",
    "\n",
    "\n",
    "\n",
    "# 2. Definir Critérios de Relevância -------------------------------------------------------------------------------------------------\n",
    "\n",
    "def tratar_dados(json_de_noticias) -> pd.DataFrame:\n",
    "    df = pd.json_normalize(json_de_noticias['articles'])\n",
    "    df['publishedAt'] = pd.to_datetime(df['publishedAt']).dt.tz_localize(None) # inserido, pq linha abaixo estava dando erro \"TypeError: Cannot use .astype to convert from timezone-aware dtype to timezone-naive dtype\"\n",
    "    df['publishedAt'] = df['publishedAt'].astype('datetime64[ms]')\n",
    "    df.fillna(\"desconhecido\", inplace=True)\n",
    "\n",
    "    # Remover noticias sem informações nas colunas: descrição, conteúdo e nome de fonte \n",
    "    df = df[~df['description'].str.contains('desconhecido', case=False)]\n",
    "    df = df[~df['content'].str.contains('desconhecido', case=False)]\n",
    "    df = df[~df['source.name'].str.contains('desconhecido', case=False)]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# 3. Cargas em Batches: ---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def armazenar_noticias(df) -> None:\n",
    "\n",
    "    # Verifica se já existe um arquivo com os dados\n",
    "    if os.path.isfile('noticias.csv'):\n",
    "        # Lê o arquivo existente\n",
    "        df_existente = carrega_armazenamento()\n",
    "\n",
    "        # Identifica as notícias repetidas\n",
    "        df_repetidas = df[df['title'].isin(df_existente['title'])]\n",
    "\n",
    "        # Exclui as notícias repetidas\n",
    "        df = df.drop(df_repetidas.index)\n",
    "\n",
    "    # Grava o dataframe em um arquivo CSV\n",
    "    df.to_csv('noticias.csv', index=0, mode='a', header=False)\n",
    "\n",
    "    return None\n",
    "\n",
    "def carrega_armazenamento():\n",
    "    df = pd.read_csv('noticias.csv', index_col=0)\n",
    "    return df \n",
    "\n",
    "\n",
    "\n",
    "# 4. Dados transformados para consulta do público final ------------------------------------------------------------------------------\n",
    "\n",
    "def qtd_noticia_ano_mes_dia(df_inteiro):\n",
    "\n",
    "    # Calcula a quantidade de notícias por ano, mês/ano e dia/mês/ano\n",
    "    df_inteiro['ano'] = df_inteiro['publishedAt'].dt.year\n",
    "    df_inteiro['mes_ano'] = df_inteiro['publishedAt'].dt.to_period('M')\n",
    "    df_inteiro['dia_mes_ano'] = df_inteiro['publishedAt'].dt.to_period('D')\n",
    "    \n",
    "    qtd_por_ano = df_inteiro['ano'].value_counts().sort_index().reset_index()\n",
    "    qtd_por_ano.columns = ['ano', 'quantidade']\n",
    "\n",
    "    qtd_por_mes = df_inteiro['mes_ano'].value_counts().sort_index().reset_index()\n",
    "    qtd_por_mes.columns = ['mes_ano', 'quantidade']\n",
    "\n",
    "    qtd_por_dia = df_inteiro['dia_mes_ano'].value_counts().sort_index().reset_index()\n",
    "    qtd_por_dia.columns = ['dia_mes_ano', 'quantidade']\n",
    "    \n",
    "    return qtd_por_ano, qtd_por_mes, qtd_por_dia \n",
    "\n",
    "def qtd_noticia_fonte_autor(df_inteiro):\n",
    "    # Calcula a quantidade de notícias por fonte\n",
    "    qtd_por_fonte = df_inteiro['source.name'].value_counts().reset_index()\n",
    "    qtd_por_fonte.columns = ['fonte', 'quantidade']\n",
    "    \n",
    "    # Calcula a quantidade de notícias por autor\n",
    "    qtd_por_autor = df_inteiro['author'].value_counts().reset_index()\n",
    "    qtd_por_autor.columns = ['autor', 'quantidade']\n",
    "    \n",
    "    return qtd_por_fonte, qtd_por_autor\n",
    "\n",
    "def qtd_aparicao_palavras_chaves(df_inteiro, palavras_chave):  \n",
    "    # Adiciona colunas para cada palavra-chave no DataFrame\n",
    "    for palavra in palavras_chave:\n",
    "        df_inteiro[palavra] = df_inteiro['description'].str.contains(fr'\\b{palavra}\\b', case=False)\n",
    "    \n",
    "    df_inteiro['ano'] = df_inteiro['publishedAt'].dt.year\n",
    "    df_inteiro['mes'] = df_inteiro['publishedAt'].dt.month # Aproveitei aqui para deixar as colunas apenas do mês e do dia, pois serão útil na função \"qtd_aparicao_palavras_chaves\"\n",
    "    df_inteiro['dia'] = df_inteiro['publishedAt'].dt.day\n",
    "\n",
    "    # Agrupa por ano, mês e dia e conta a quantidade de aparições de cada palavra-chave\n",
    "    df_inteiro = df_inteiro.groupby(['ano', 'mes', 'dia'])[palavras_chave].sum().reset_index()\n",
    "    \n",
    "    # Cria um dataframe com as colunas 'ano', 'mes', 'dia' e a contagem de cada palavra-chave\n",
    "    df_contagem_palavras_chave = df_inteiro.melt(id_vars=['ano', 'mes', 'dia'], var_name='palavra_chave', value_name='quantidade')\n",
    "    \n",
    "    return df_contagem_palavras_chave   \n",
    "\n",
    "def armazenar_dados_estatisticos(qtd_por_ano, qtd_por_mes, qtd_por_dia, qtd_por_fonte, qtd_por_autor, df_contagem_palavras_chave):\n",
    "    print(\"Quantidade de notícias por ano:\")\n",
    "    print(qtd_por_ano)\n",
    "\n",
    "    print(\"\\nQuantidade de notícias por mês:\")\n",
    "    print(qtd_por_mes)\n",
    "\n",
    "    print(\"\\nQuantidade de notícias por dia:\")\n",
    "    print(qtd_por_dia)\n",
    "\n",
    "    print(\"\\nQuantidade de notícias por fonte:\")\n",
    "    print(qtd_por_fonte)\n",
    "\n",
    "    print(\"\\nQuantidade de notícias por autor:\")\n",
    "    print(qtd_por_autor)\n",
    "\n",
    "    print(\"\\nContagem de aparições das palavras-chave:\")\n",
    "    print(df_contagem_palavras_chave)\n",
    "    \n",
    "    estatisticas = pd.concat([qtd_por_ano, qtd_por_mes, qtd_por_dia, qtd_por_fonte, qtd_por_autor, df_contagem_palavras_chave], axis=1)\n",
    "    estatisticas.to_csv('resultado_final.csv', index = False)\n",
    "    #estatisticas.to_excel('resultado_final.xlsx', index = False)\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Descentralizando...\n",
    "\n",
    "def procedimento_hora_hora():\n",
    "    # 1. Consumo de dados com a News API\n",
    "    response = fazer_a_request_1(API_KEY, PALAVRAS_CHAVES_GERAIS)\n",
    "\n",
    "    # 2. Definir Critérios de Relevância\n",
    "    df_agora = tratar_dados(response)\n",
    "\n",
    "    # 3. Cargas em Batches:\n",
    "    armazenar_noticias(df_agora)\n",
    "    \n",
    "\n",
    "def procedimento_diario():\n",
    "    # 4. Dados transformados para consulta do público final\n",
    "    df_inteiro = carrega_armazenamento()\n",
    "    df_inteiro[\"publishedAt\"] = pd.to_datetime(df_inteiro[\"publishedAt\"])\n",
    "    a, b, c = qtd_noticia_ano_mes_dia(df_inteiro)\n",
    "    d, e = qtd_noticia_fonte_autor(df_inteiro)\n",
    "    f = qtd_aparicao_palavras_chaves(df_inteiro, PALAVRAS_CHAVES_ESPECIFICAS)\n",
    "    armazenar_dados_estatisticos(a , b , c, d, e, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chaves retornadas: dict_keys(['status', 'totalResults', 'articles'])\n",
      "Status: ok\n",
      "Total de resultados: 11\n",
      "Exemplo de artigo: {'source': {'id': None, 'name': 'Metropoles.com'}, 'author': 'Bethânia Nunes', 'title': 'IA: entenda como ela pode ser usada em benefício da saúde', 'description': 'A inteligência artificial (IA) já está presente na medicina no atendimento, tratamento e apoio ao paciente. Veja alguns exemplos de seu uso', 'url': 'https://www.metropoles.com/saude/inteligencia-artificial-uso-saude', 'urlToImage': 'https://uploads.metropoles.com/wp-content/uploads/2023/07/13112836/Mulher-ressonancia-cerebro-inteligencia-artificial.jpg', 'publishedAt': '2023-09-30T05:02:28Z', 'content': 'A inteligência artificial (IA) vem provocando uma grande transformação na medicina, e é provável que ela esteja em um dos consultórios que você frequenta. Embora o termo ainda pareça abstrato para gr… [+4477 chars]'}\n"
     ]
    }
   ],
   "source": [
    "# Instanciar objeto\n",
    "response = fazer_a_request_1()\n",
    "# Exibir chaves do dicionário\n",
    "print(f'Chaves retornadas: {response.keys()}')\n",
    "# Valores do dicionário\n",
    "print(f\"Status: {response['status']}\")\n",
    "print(f\"Total de resultados: {response['totalResults']}\")\n",
    "print(f\"Exemplo de artigo: {response['articles'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prova real que ambos os metodos de request dão o mesmo resultado\n",
    "print(fazer_a_request_1() == fazer_a_request_2())\n",
    "# teste do tratamento de dados\n",
    "df = tratar_dados(response)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "procedimento_hora_hora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de notícias por ano:\n",
      "    ano  quantidade\n",
      "0  2023           9\n",
      "\n",
      "Quantidade de notícias por mês:\n",
      "   mes_ano  quantidade\n",
      "0  2023-09           8\n",
      "1  2023-10           1\n",
      "\n",
      "Quantidade de notícias por dia:\n",
      "  dia_mes_ano  quantidade\n",
      "0  2023-09-13           1\n",
      "1  2023-09-14           1\n",
      "2  2023-09-19           1\n",
      "3  2023-09-22           1\n",
      "4  2023-09-23           1\n",
      "5  2023-09-25           1\n",
      "6  2023-09-29           1\n",
      "7  2023-09-30           1\n",
      "8  2023-10-02           1\n",
      "\n",
      "Quantidade de notícias por fonte:\n",
      "                 fonte  quantidade\n",
      "0         Terra.com.br           2\n",
      "1       Metropoles.com           2\n",
      "2                Globo           2\n",
      "3           Uol.com.br           1\n",
      "4         Abril.com.br           1\n",
      "5  Catracalivre.com.br           1\n",
      "\n",
      "Quantidade de notícias por autor:\n",
      "              autor  quantidade\n",
      "0      desconhecido           3\n",
      "1           Redação           2\n",
      "2      Saúde em Dia           1\n",
      "3    Bethânia Nunes           1\n",
      "4  Estadão Conteúdo           1\n",
      "5  Samara Schwingel           1\n",
      "\n",
      "Contagem de aparições das palavras-chave:\n",
      "     ano  mes  dia   palavra_chave  quantidade\n",
      "0   2023    9   13         terapia           0\n",
      "1   2023    9   14         terapia           0\n",
      "2   2023    9   19         terapia           0\n",
      "3   2023    9   22         terapia           0\n",
      "4   2023    9   23         terapia           0\n",
      "5   2023    9   25         terapia           0\n",
      "6   2023    9   29         terapia           0\n",
      "7   2023    9   30         terapia           0\n",
      "8   2023   10    2         terapia           0\n",
      "9   2023    9   13  sequenciamento           0\n",
      "10  2023    9   14  sequenciamento           0\n",
      "11  2023    9   19  sequenciamento           0\n",
      "12  2023    9   22  sequenciamento           0\n",
      "13  2023    9   23  sequenciamento           0\n",
      "14  2023    9   25  sequenciamento           0\n",
      "15  2023    9   29  sequenciamento           0\n",
      "16  2023    9   30  sequenciamento           0\n",
      "17  2023   10    2  sequenciamento           0\n",
      "18  2023    9   13          doença           0\n",
      "19  2023    9   14          doença           0\n",
      "20  2023    9   19          doença           0\n",
      "21  2023    9   22          doença           0\n",
      "22  2023    9   23          doença           0\n",
      "23  2023    9   25          doença           0\n",
      "24  2023    9   29          doença           0\n",
      "25  2023    9   30          doença           0\n",
      "26  2023   10    2          doença           1\n"
     ]
    }
   ],
   "source": [
    "procedimento_diario()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de notícias por ano:\n",
      "    ano  quantidade\n",
      "0  2023           9\n",
      "\n",
      "Quantidade de notícias por mês:\n",
      "   mes_ano  quantidade\n",
      "0  2023-09           8\n",
      "1  2023-10           1\n",
      "\n",
      "Quantidade de notícias por dia:\n",
      "  dia_mes_ano  quantidade\n",
      "0  2023-09-13           1\n",
      "1  2023-09-14           1\n",
      "2  2023-09-19           1\n",
      "3  2023-09-22           1\n",
      "4  2023-09-23           1\n",
      "5  2023-09-25           1\n",
      "6  2023-09-29           1\n",
      "7  2023-09-30           1\n",
      "8  2023-10-02           1\n",
      "\n",
      "Quantidade de notícias por fonte:\n",
      "                 fonte  quantidade\n",
      "0         Terra.com.br           2\n",
      "1       Metropoles.com           2\n",
      "2                Globo           2\n",
      "3           Uol.com.br           1\n",
      "4         Abril.com.br           1\n",
      "5  Catracalivre.com.br           1\n",
      "\n",
      "Quantidade de notícias por autor:\n",
      "              autor  quantidade\n",
      "0      desconhecido           3\n",
      "1           Redação           2\n",
      "2      Saúde em Dia           1\n",
      "3    Bethânia Nunes           1\n",
      "4  Estadão Conteúdo           1\n",
      "5  Samara Schwingel           1\n",
      "\n",
      "Contagem de aparições das palavras-chave:\n",
      "     ano  mes  dia   palavra_chave  quantidade\n",
      "0   2023    9   13         terapia           0\n",
      "1   2023    9   14         terapia           0\n",
      "2   2023    9   19         terapia           0\n",
      "3   2023    9   22         terapia           0\n",
      "4   2023    9   23         terapia           0\n",
      "5   2023    9   25         terapia           0\n",
      "6   2023    9   29         terapia           0\n",
      "7   2023    9   30         terapia           0\n",
      "8   2023   10    2         terapia           0\n",
      "9   2023    9   13  sequenciamento           0\n",
      "10  2023    9   14  sequenciamento           0\n",
      "11  2023    9   19  sequenciamento           0\n",
      "12  2023    9   22  sequenciamento           0\n",
      "13  2023    9   23  sequenciamento           0\n",
      "14  2023    9   25  sequenciamento           0\n",
      "15  2023    9   29  sequenciamento           0\n",
      "16  2023    9   30  sequenciamento           0\n",
      "17  2023   10    2  sequenciamento           0\n",
      "18  2023    9   13          doença           0\n",
      "19  2023    9   14          doença           0\n",
      "20  2023    9   19          doença           0\n",
      "21  2023    9   22          doença           0\n",
      "22  2023    9   23          doença           0\n",
      "23  2023    9   25          doença           0\n",
      "24  2023    9   29          doença           0\n",
      "25  2023    9   30          doença           0\n",
      "26  2023   10    2          doença           1\n"
     ]
    }
   ],
   "source": [
    "from newsapi import NewsApiClient\n",
    "import requests\n",
    "import time\n",
    "\n",
    "API_KEY = '9a77398581d74beebbd29dbebd159a53'\n",
    "PALAVRAS_CHAVES_GERAIS = '(genômica OR genômico) AND (terapia OR sequenciamento OR doença)'\n",
    "PALAVRAS_CHAVES_ESPECIFICAS = ['terapia', 'sequenciamento', 'doença']\n",
    "\n",
    "INTERVALO = 2 # 3600 segundos = 1 hora\n",
    "i = 22\n",
    "\n",
    "while True:\n",
    "\n",
    "    procedimento_hora_hora()\n",
    "    if i==23: # i=23 referente as 24h do dia\n",
    "        procedimento_diario()\n",
    "        i = 0\n",
    "        break   \n",
    "    else:\n",
    "        i += 1\n",
    "        \n",
    "    time.sleep(INTERVALO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [author, title, description, url, urlToImage, publishedAt, content, source.id, source.name]\n",
      "Index: []\n",
      "Quantidade de notícias por ano:\n",
      "Empty DataFrame\n",
      "Columns: [ano, quantidade]\n",
      "Index: []\n",
      "\n",
      "Quantidade de notícias por mês:\n",
      "Empty DataFrame\n",
      "Columns: [mes_ano, quantidade]\n",
      "Index: []\n",
      "\n",
      "Quantidade de notícias por dia:\n",
      "Empty DataFrame\n",
      "Columns: [dia_mes_ano, quantidade]\n",
      "Index: []\n",
      "\n",
      "Quantidade de notícias por fonte:\n",
      "Empty DataFrame\n",
      "Columns: [fonte, quantidade]\n",
      "Index: []\n",
      "\n",
      "Quantidade de notícias por autor:\n",
      "Empty DataFrame\n",
      "Columns: [autor, quantidade]\n",
      "Index: []\n",
      "\n",
      "Contagem de aparições das palavras-chave:\n",
      "Empty DataFrame\n",
      "Columns: [ano, mes, dia, palavra_chave, quantidade]\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Google Drive\\CoisasMinhas\\ADA\\ADA-Santander-1010\\modulo 2\\ada_santander\\Modulo 4 - Extracao de dados I\\Projeto - Extração de Dados I\\src\\projeto.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Google%20Drive/CoisasMinhas/ADA/ADA-Santander-1010/modulo%202/ada_santander/Modulo%204%20-%20Extracao%20de%20dados%20I/Projeto%20-%20Extra%C3%A7%C3%A3o%20de%20Dados%20I/src/projeto.ipynb#X21sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Google%20Drive/CoisasMinhas/ADA/ADA-Santander-1010/modulo%202/ada_santander/Modulo%204%20-%20Extracao%20de%20dados%20I/Projeto%20-%20Extra%C3%A7%C3%A3o%20de%20Dados%20I/src/projeto.ipynb#X21sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Google%20Drive/CoisasMinhas/ADA/ADA-Santander-1010/modulo%202/ada_santander/Modulo%204%20-%20Extracao%20de%20dados%20I/Projeto%20-%20Extra%C3%A7%C3%A3o%20de%20Dados%20I/src/projeto.ipynb#X21sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m time\u001b[39m.\u001b[39;49msleep(INTERVALO)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from newsapi import NewsApiClient\n",
    "import requests\n",
    "import time\n",
    "\n",
    "API_KEY = '9a77398581d74beebbd29dbebd159a53'\n",
    "PALAVRAS_CHAVES_GERAIS = '(genômica OR genômico) AND (terapia OR sequenciamento OR doença)'\n",
    "PALAVRAS_CHAVES_ESPECIFICAS = ['terapia', 'sequenciamento', 'doença']\n",
    "\n",
    "INTERVALO = 3600 # 3600 segundos = 1 hora\n",
    "i = 0\n",
    "\n",
    "while True:\n",
    "\n",
    "    procedimento_hora_hora()\n",
    "    if i==23: # i=23 referente as 24h do dia\n",
    "        procedimento_diario()\n",
    "        i = 0       \n",
    "    else:\n",
    "        i += 1\n",
    "        \n",
    "    time.sleep(INTERVALO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
